import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# ==============================
# CONFIG
# ==============================
VECTOR_STORE_PATH = "vector_stores/faiss_index"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
LLM_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# ==============================
# LOAD EMBEDDINGS
# ==============================
print("üîÑ Loading embedding model...")

embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
)

# ==============================
# LOAD VECTOR STORE
# ==============================
print("üîÑ Loading FAISS index...")

vectorstore = FAISS.load_local(
    VECTOR_STORE_PATH,
    embeddings,
    allow_dangerous_deserialization=True
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# ==============================
# LOAD LLM
# ==============================
print("üîÑ Loading LLM model... (This may take time)")

tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)
model = AutoModelForCausalLM.from_pretrained(
    LLM_MODEL,
    torch_dtype="auto",
    device_map="auto"
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False
)

print("‚úÖ AI-Prof RAG Ready!\n")

# ==============================
# CHAT LOOP
# ==============================
while True:
    query = input("Ask question (type exit to stop): ")

    if query.lower() == "exit":
        break

    # 1Ô∏è‚É£ Retrieve relevant documents
    docs = retriever.invoke(query)
    context = "\n\n".join([doc.page_content for doc in docs])

    # 2Ô∏è‚É£ Create RAG prompt
    prompt = f"""
You are an AI professor.

Answer the question in 3-4 clear sentences.

Do NOT:
- Repeat the question
- Generate additional questions
- Continue the document
- Include the context in the answer

Context:
{context}

Question:
{query}

Answer:
"""

    # 3Ô∏è‚É£ Generate response
    response = pipe(
        prompt,
        max_new_tokens=150,
        temperature=0.1,
        do_sample=False
    )
    output = response[0]["generated_text"]
    print("\nüîç LLM Output:\n", output)
# Remove prompt portion
    answer = output[len(prompt):]
    
    print("\nüìò Answer:\n", answer.strip())
    # print("\nüìò Answer:\n")
    # print(response[0]["generated_text"])
    # print("\n" + "-" * 60 + "\n")